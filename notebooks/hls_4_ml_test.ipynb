{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\isakb\\miniforge3\\envs\\dml_cpu\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from src.gnn_models import GNN_7\n",
    "import hls4ml\n",
    "from torch_geometric.utils import to_dense_adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomGraphLayer(nn.Module):\n",
    "    def __init__(self, input_features, output_features):\n",
    "        super().__init__()\n",
    "        self.weight_node = nn.Linear(input_features, output_features, bias=False)\n",
    "        self.weight_adj = nn.Linear(input_features, output_features, bias=True)\n",
    "\n",
    "    def forward(self, node_features, Adj, batch):\n",
    "        new_sample_index = torch.where(batch[:-1] != batch[1:])[0] + 1\n",
    "        new_sample_index = torch.cat(\n",
    "            (torch.tensor([0], device=\"cpu\"), new_sample_index)\n",
    "        )\n",
    "        new_sample_index = torch.cat(\n",
    "            (new_sample_index, torch.tensor([len(batch)], device=\"cpu\"))\n",
    "        )\n",
    "        num_nodes_list = new_sample_index[1:] - new_sample_index[:-1]\n",
    "        for i, num_nodes in enumerate(num_nodes_list):\n",
    "            # reshape long list of flattened Adjacency matrices to one big block\n",
    "            # diagonal matrix over multiple graphs\n",
    "            square_Adj = Adj[: num_nodes**2, :].reshape(num_nodes, num_nodes)\n",
    "            Adj = Adj[num_nodes**2 :, :]\n",
    "            if i == 0:\n",
    "                block_Adj = square_Adj\n",
    "            else:\n",
    "                block_Adj = torch.block_diag(block_Adj, square_Adj)\n",
    "        node_term = self.weight_node(node_features)\n",
    "        adjacency_sum = torch.matmul(block_Adj, node_features)\n",
    "        adjacency_term = self.weight_adj(adjacency_sum)\n",
    "        new_node_features = node_term + adjacency_term\n",
    "\n",
    "        return new_node_features\n",
    "\n",
    "class PMatMul(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, x1, x2):\n",
    "        return torch.matmul(x1, x2)\n",
    "\n",
    "class HMatMul(hls4ml.model.layers.Layer):\n",
    "    \n",
    "    def initialize(self):\n",
    "        inp = self.get_input_variable()\n",
    "        shape = inp.shape\n",
    "        dims = inp.dim_names\n",
    "        self.add_output_variable(shape, dims)\n",
    "    \n",
    "class CustomGraphConv(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_features, output_features):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.matmul = PMatMul()\n",
    "        self.weight_node = nn.Linear(input_features, output_features, bias=False)\n",
    "        self.weight_adj = nn.Linear(input_features, output_features, bias=True)\n",
    "        \n",
    "    def forward(self, x, adj):\n",
    "        \n",
    "        node_term = self.weight_node(x)\n",
    "        \n",
    "        adjaceny_sum = self.matmul(adj, x)\n",
    "        adjaceny_term = self.weight_adj(adjaceny_sum)\n",
    "        \n",
    "        return node_term + adjaceny_term\n",
    "        \n",
    "class GraphWTorchNet(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_channels_GCN=[32, 128, 256, 512, 512, 256, 256],\n",
    "        hidden_channels_MLP=[256, 128, 64],\n",
    "        num_node_features=5,\n",
    "        num_classes=1,\n",
    "        manual_seed=12345,\n",
    "    ):\n",
    "        # num_classes is 1 for each head\n",
    "        super().__init__()\n",
    "        if manual_seed is not None:\n",
    "            torch.manual_seed(manual_seed)\n",
    "\n",
    "        # Activation\n",
    "        self.activation = nn.ReLU()\n",
    "        \n",
    "        # GCN layers\n",
    "        channels = [num_node_features] + hidden_channels_GCN\n",
    "        self.graph_layers = nn.ModuleList(\n",
    "            [\n",
    "                CustomGraphConv(in_channels, out_channels)\n",
    "                for (in_channels, out_channels) in zip(channels[:-1], channels[1:])\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Dense layers\n",
    "        channels = hidden_channels_GCN[-1:] + hidden_channels_MLP\n",
    "        self.dense_layers = nn.ModuleList(\n",
    "            [\n",
    "                nn.Linear(in_channels, out_channels)\n",
    "                for (in_channels, out_channels) in zip(channels[:-1], channels[1:])\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Output later\n",
    "        self.output_layer = nn.Linear(hidden_channels_MLP[-1], num_classes)\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        \n",
    "        #  node embeddings\n",
    "        for layer in self.graph_layers:\n",
    "            x = layer(x, adj)\n",
    "            x = self.activation(x)\n",
    "\n",
    "        # global mean pool\n",
    "        x = x.mean(axis=0)\n",
    "\n",
    "        for layer in self.dense_layers:\n",
    "            x = layer(x)\n",
    "            x = self.activation(x)\n",
    "\n",
    "        # output\n",
    "        x = self.output_layer(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "def parse_matmul_layer(torch_layer, input_names, input_shapes, data_reader):\n",
    "    layer = {}\n",
    "    layer[\"class_name\"] = \"HMatMul\"\n",
    "    layer[\"name\"] = torch_layer[\"config\"][\"name\"]\n",
    "    layer[\"n_in\"] = input_shapes[0][1]\n",
    "    \n",
    "    if input_names is not None:\n",
    "        layer[\"inputs\"] = input_names\n",
    "    \n",
    "    return layer, [shape for shape in input_shapes[0]]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_dict(d, indent=0):\n",
    "    for key, value in d.items():\n",
    "        print('  ' * indent + str(key), end='')\n",
    "        if isinstance(value, dict):\n",
    "            print()\n",
    "            print_dict(value, indent + 1)\n",
    "        else:\n",
    "            print(':' + ' ' * (20 - len(key) - 2 * indent) + str(value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GraphWTorchNet(\n",
       "  (activation): ReLU()\n",
       "  (graph_layers): ModuleList(\n",
       "    (0): CustomGraphConv(\n",
       "      (matmul): PMatMul()\n",
       "      (weight_node): Linear(in_features=5, out_features=32, bias=False)\n",
       "      (weight_adj): Linear(in_features=5, out_features=32, bias=True)\n",
       "    )\n",
       "    (1): CustomGraphConv(\n",
       "      (matmul): PMatMul()\n",
       "      (weight_node): Linear(in_features=32, out_features=128, bias=False)\n",
       "      (weight_adj): Linear(in_features=32, out_features=128, bias=True)\n",
       "    )\n",
       "    (2): CustomGraphConv(\n",
       "      (matmul): PMatMul()\n",
       "      (weight_node): Linear(in_features=128, out_features=256, bias=False)\n",
       "      (weight_adj): Linear(in_features=128, out_features=256, bias=True)\n",
       "    )\n",
       "    (3): CustomGraphConv(\n",
       "      (matmul): PMatMul()\n",
       "      (weight_node): Linear(in_features=256, out_features=512, bias=False)\n",
       "      (weight_adj): Linear(in_features=256, out_features=512, bias=True)\n",
       "    )\n",
       "    (4): CustomGraphConv(\n",
       "      (matmul): PMatMul()\n",
       "      (weight_node): Linear(in_features=512, out_features=512, bias=False)\n",
       "      (weight_adj): Linear(in_features=512, out_features=512, bias=True)\n",
       "    )\n",
       "    (5): CustomGraphConv(\n",
       "      (matmul): PMatMul()\n",
       "      (weight_node): Linear(in_features=512, out_features=256, bias=False)\n",
       "      (weight_adj): Linear(in_features=512, out_features=256, bias=True)\n",
       "    )\n",
       "    (6): CustomGraphConv(\n",
       "      (matmul): PMatMul()\n",
       "      (weight_node): Linear(in_features=256, out_features=256, bias=False)\n",
       "      (weight_adj): Linear(in_features=256, out_features=256, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (dense_layers): ModuleList(\n",
       "    (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (1): Linear(in_features=256, out_features=128, bias=True)\n",
       "    (2): Linear(in_features=128, out_features=64, bias=True)\n",
       "  )\n",
       "  (output_layer): Linear(in_features=64, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GraphWTorchNet()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "matmul_config_template = \"\"\"struct config{index} : nnet::matmul_config {{\n",
    "    static const unsigned n_in = {n_in};\n",
    "}};\\n\"\"\"\n",
    "\n",
    "matmul_function_template = 'nnet::product<{x_t}, {y_t}>({x}, {y});'\n",
    "matmul_include_list = ['nnet_utils/nnet_matmul.h']\n",
    "\n",
    "class HMatMulConfigTemplate(hls4ml.backends.template.LayerConfigTemplate):\n",
    "    def __init__(self):\n",
    "        super().__init__(HMatMul)\n",
    "        self.template = matmul_config_template\n",
    "\n",
    "    def format(self, node):\n",
    "        params = self._default_config_params(node)\n",
    "        return self.template.format(**params)\n",
    "\n",
    "\n",
    "class HMatMulFunctionTemplate(hls4ml.backends.template.FunctionCallTemplate):\n",
    "    def __init__(self):\n",
    "        super().__init__(HMatMul, include_header=matmul_include_list)\n",
    "        self.template = matmul_function_template\n",
    "\n",
    "    def format(self, node):\n",
    "        params = self._default_function_params(node)\n",
    "        return self.template.format(**params)\n",
    "    \n",
    "# Register the converter for custom Keras layer\n",
    "hls4ml.converters.register_pytorch_layer_handler('PMatMul', parse_matmul_layer)\n",
    "\n",
    "# Register the hls4ml's IR layer\n",
    "hls4ml.model.layers.register_layer('HMatMul', HMatMul)\n",
    "\n",
    "\n",
    "# Register the optimization passes (if any)\n",
    "backend = hls4ml.backends.get_backend(\"Vivado\")\n",
    "# backend.register_pass('remove_duplicate_reverse', RemoveDuplicateReverse, flow=f'{backend_id.lower()}:optimize')\n",
    "\n",
    "# Register template passes for the given backend\n",
    "backend.register_template(HMatMulConfigTemplate)\n",
    "backend.register_template(HMatMulFunctionTemplate)\n",
    "\n",
    "# Register HLS implementation\n",
    "path = Path(\"C:/Users/isakb/miniforge3/envs/dml_cpu/Lib/site-packages/hls4ml/backends/templates/vivado/nnet_utils/nnet_mult.h\")\n",
    "backend.register_source(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interpreting Model ...\n",
      "Topology:\n",
      "Layer name: graph_layers_0_weight_node, layer type: Dense, input shape: [[100, 5]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\isakb\\miniforge3\\envs\\dml_cpu\\Lib\\site-packages\\torch\\overrides.py:110: UserWarning: 'has_cuda' is deprecated, please use 'torch.backends.cuda.is_built()'\n",
      "  torch.has_cuda,\n",
      "c:\\Users\\isakb\\miniforge3\\envs\\dml_cpu\\Lib\\site-packages\\torch\\overrides.py:111: UserWarning: 'has_cudnn' is deprecated, please use 'torch.backends.cudnn.is_available()'\n",
      "  torch.has_cudnn,\n",
      "c:\\Users\\isakb\\miniforge3\\envs\\dml_cpu\\Lib\\site-packages\\torch\\overrides.py:117: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  torch.has_mps,\n",
      "c:\\Users\\isakb\\miniforge3\\envs\\dml_cpu\\Lib\\site-packages\\torch\\overrides.py:118: UserWarning: 'has_mkldnn' is deprecated, please use 'torch.backends.mkldnn.is_available()'\n",
      "  torch.has_mkldnn,\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Unsupported function matmul",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\isakb\\Documents\\M.Sc Physics\\Code\\GNN_decoder_quant\\notebooks\\hls_4_ml_test.ipynb Cell 6\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/isakb/Documents/M.Sc%20Physics/Code/GNN_decoder_quant/notebooks/hls_4_ml_test.ipynb#W4sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m config \u001b[39m=\u001b[39m hls4ml\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mconfig_from_pytorch_model(model)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/isakb/Documents/M.Sc%20Physics/Code/GNN_decoder_quant/notebooks/hls_4_ml_test.ipynb#W4sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m input_shapes \u001b[39m=\u001b[39m [(\u001b[39m100\u001b[39m, \u001b[39m5\u001b[39m), (\u001b[39m400\u001b[39m, \u001b[39m400\u001b[39m)]\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/isakb/Documents/M.Sc%20Physics/Code/GNN_decoder_quant/notebooks/hls_4_ml_test.ipynb#W4sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m hls_model \u001b[39m=\u001b[39m hls4ml\u001b[39m.\u001b[39;49mconverters\u001b[39m.\u001b[39;49mconvert_from_pytorch_model(model, input_shape\u001b[39m=\u001b[39;49minput_shapes, backend\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mVivado\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\isakb\\miniforge3\\envs\\dml_cpu\\Lib\\site-packages\\hls4ml\\converters\\__init__.py:305\u001b[0m, in \u001b[0;36mconvert_from_pytorch_model\u001b[1;34m(model, input_shape, output_dir, project_name, input_data_tb, output_data_tb, backend, hls_config, **kwargs)\u001b[0m\n\u001b[0;32m    301\u001b[0m config[\u001b[39m'\u001b[39m\u001b[39mHLSConfig\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mModel\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m _check_model_config(model_config)\n\u001b[0;32m    303\u001b[0m _check_hls_config(config, hls_config)\n\u001b[1;32m--> 305\u001b[0m \u001b[39mreturn\u001b[39;00m pytorch_to_hls(config)\n",
      "File \u001b[1;32mc:\\Users\\isakb\\miniforge3\\envs\\dml_cpu\\Lib\\site-packages\\hls4ml\\converters\\pytorch_to_hls.py:257\u001b[0m, in \u001b[0;36mpytorch_to_hls\u001b[1;34m(config)\u001b[0m\n\u001b[0;32m    255\u001b[0m \u001b[39m# only a limited number of functions are supported\u001b[39;00m\n\u001b[0;32m    256\u001b[0m \u001b[39mif\u001b[39;00m operation \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m supported_layers:\n\u001b[1;32m--> 257\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mException\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mUnsupported function \u001b[39m\u001b[39m{\u001b[39;00moperation\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m    258\u001b[0m \u001b[39mif\u001b[39;00m operation \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mPReLU\u001b[39m\u001b[39m'\u001b[39m \u001b[39mor\u001b[39;00m operation \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mbatch_norm\u001b[39m\u001b[39m'\u001b[39m \u001b[39mor\u001b[39;00m operation \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mconv1d\u001b[39m\u001b[39m'\u001b[39m \u001b[39mor\u001b[39;00m operation \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mconv2d\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    259\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mException\u001b[39;00m(\n\u001b[0;32m    260\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mFunction \u001b[39m\u001b[39m{\u001b[39;00moperation\u001b[39m}\u001b[39;00m\u001b[39m cannot be parsed as torch.nn.functional. Use the torch.nn implementation instead\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    261\u001b[0m     )\n",
      "\u001b[1;31mException\u001b[0m: Unsupported function matmul"
     ]
    }
   ],
   "source": [
    "config = hls4ml.utils.config_from_pytorch_model(model)\n",
    "input_shapes = [(100, 5), (400, 400)]\n",
    "hls_model = hls4ml.converters.convert_from_pytorch_model(model, input_shape=input_shapes, backend=\"Vivado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'nnet_utils\\\\nnet_mult.h': WindowsPath('C:/Users/isakb/miniforge3/envs/dml_cpu/Lib/site-packages/hls4ml/backends/templates/vivado/nnet_utils/nnet_mult.h')}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "backend.get_custom_source()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dml_cpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
